# -*- coding: utf-8 -*-
"""Plant_Seed_CNN_Project1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cudeUOuJ6lumvfG2VcPIVMuM80iSQTD1
"""

from google.colab import drive
drive.mount('/content/drive')

"""Steps and Milestones (100%):

 Setup Environment and Load Necessary Packages (5%)

 Data Preparation (40%)

o Loading Data (5%)

o Cleaning Data (10%)

o Data Representation & Feature Engineering (If Any) (15%)

o Creating Train and Validation Set (10%)

 Model Creation (30%)

o Write & Configure Model (10%)

o Compile Model (10%)

o Build Model & Checking Summary (10%)

 Training and Evaluation (25%)

o Run Multiple Experiments (10%)


o Reason & Visualize Model Performance (5%)

o Evaluate Model on Test Set (10%)
"""

import tensorflow as tf

tf.__version__

# there were 2 folders named train and test.
# train folder had 12 sub-folder each named by a class of plants
# uploaded the zip files off the two folders in google drive

from zipfile import ZipFile

# extracting data from train folder

with ZipFile('/content/drive/My Drive/Conv-Project-1/plant-seedlings-classification.zip', 'r') as z:
  z.extractall()

# extracting data from test folder

import os
os.chdir('/content/drive/My Drive/Conv-Project-1')

filename='plant-seedlings-classification.zip'

with ZipFile(filename, 'r') as z:
  z.extractall()

import os
from skimage.io import imread
from skimage.transform import resize 
# resize perform Gaussian smoothing to avoid aliasing artifacts . once the images were read from skimage and resize was performed there was a problem in converting the list of array (X_train) into np.array with error that array of size 128X128X3 cant be resized into 128X128 . OPEN CV does not have that problem.

# processing test data

X_test=[]
X_test_name=[]
os.chdir('test') # changing directory

os.listdir()[0] # first image in test folder

image_1 = imread(os.listdir()[0]) # reading the first image
print(image_1.shape)
print(image_1)

import matplotlib.pyplot as plt
plt.imshow(image_1) # the first test image

os.listdir()[0:10] # first ten images in test folder

for i in os.listdir()[0:10]:
  image_example = imread(i)
  print(image_example.shape) # printing their shape

# Images are of different shape so reshaping all images into a common size is important

import cv2

for i in os.listdir():

    dummy = cv2.imread(i) # reading all test images
    dummy = cv2.resize(dummy,(128,128)) #resize to have all the images of same size  
    X_test.append(dummy)
    X_test_name.append(i)

X_test[0:3] # first three converted images

X_test[0].shape

X_test[2].shape

plt.imshow(X_test[2])

X_test_name[0:10]

# processing train data

os.chdir('/content/train/Black-grass') # having a look at one of the subfolders of the train folder

os.getcwd()

os.listdir()[0] # first image if the sub folder

image_1 = imread(os.listdir()[0]) # reading and printing the shape
print(image_1.shape)
print(image_1)

plt.imshow(image_1)

os.listdir()[0:10]

for i in os.listdir()[0:10]:
  image_example = imread(i)
  print(image_example.shape)

# the sizes are different, must be converted into similar size

X_train=[]
Y_train=[]
os.chdir('/content/train')

ls

os.listdir()

#There are 12 distinct categories. Let see on how the train data is splitted among these categories: 
for i in os.listdir():
  path, dirs, files = next(os.walk(i))
  print (i + " : " + str(len(files)))

# highly imbalance dataset. Image augmentation must be performed otherwise the model will be bias towards the class having higher samples if the model is fed with those imabalnced data

import warnings
warnings.filterwarnings("ignore")

import random
from scipy import ndarray
import skimage as sk
from skimage import transform
from skimage import util

def random_rotation(image_array: ndarray):
    # pick a random degree of rotation between 25% on the left and 25% on the right
    random_degree = random.uniform(-25, 25)
    return sk.transform.rotate(image_array, random_degree)

def random_noise(image_array: ndarray):
    # add random noise to the image
    return sk.util.random_noise(image_array)

def horizontal_flip(image_array: ndarray):
    # horizontal flip doesn't need skimage, it's easy as flipping the image array of pixels !
    return image_array[:, ::-1]
  
# dictionary of the transformations we defined earlier
available_transformations = {
    'rotate': random_rotation,
    'noise': random_noise,
    'horizontal_flip': horizontal_flip
}

#The data is imbalanced among categories. Lets balance it by augmentation.

from skimage import io

#Lets generated new images such that each category have same number of images, to eradicate imbalance and bias.
num_files_desired = 700
num_files = 0

for i in os.listdir():
    if (os.path.isdir(i)):
            Images = []
            folder_path = "/content/train/" + i
            for j in os.listdir(i):
                Images.append(folder_path + "/" + j)
            
            Img_cnt = len(Images)
            Cnt_diff = num_files_desired - Img_cnt
            num_generated_files = 0
            print ("In " + i + ", " + str(Cnt_diff) + " new images will be added by augmentation...")
            while num_generated_files < Cnt_diff:
                # random image from the folder
                image_path = random.choice(Images)
                # read image as an two dimensional array of pixels
                image_to_transform = sk.io.imread(image_path)

                # random num of transformation to apply
                num_transformations_to_apply = random.randint(1, len(available_transformations))

                num_transformations = 0
                transformed_image = None
                while num_transformations <= num_transformations_to_apply:
                    # random transformation to apply for a single image
                    key = random.choice(list(available_transformations))
                    transformed_image = available_transformations[key](image_to_transform)
                    num_transformations += 1

                    new_file_path = '%s/augmented_image_%s.png' % (folder_path, num_files)

                    # write image to the disk
                    io.imsave(new_file_path, transformed_image)
                    num_generated_files += 1
                    num_files += 1

# post augmentation 
for i in os.listdir():
  path, dirs, files = next(os.walk(i))
  print (i + " : " + str(len(files)))

for i in os.listdir():
    print("class-",i)
    if (os.path.isdir(i)):
            for j in os.listdir(i):
              try:
                    dummy = cv2.imread( i + "/" + j)
                    dummy = cv2.resize(dummy,(128,128))
                    X_train.append(dummy)
                    Y_train.append(i)
              except Exception as e:
                    print(e)

X_train[0:5]

plt.imshow(X_train[10])
print(X_train[10].shape)

plt.imshow(X_train[50])
print(X_train[50].shape)

Y_train[0:5]

print ("No. of images in X_train: ", len(X_train))
print ("No. of images in X_test: ", len(X_test))
print ("No. of values in y_train: ", len(Y_train))

print ("Shape of an image in X_train: ", X_train[0].shape)
print ("Shape of an image in X_test: ", X_test[0].shape)

#Get lable encoding for Y_train

from sklearn import preprocessing

le = preprocessing.LabelEncoder()
le.fit(Y_train)
Y_train = le.transform(Y_train)

import numpy as np

print("Total Plant categories (Unique Target): ", len(np.unique(Y_train)))

y_train = tf.keras.utils.to_categorical(Y_train, num_classes=12)

y_train = np.array(y_train)
X_train = np.array(X_train)

#performing train test split

from sklearn.model_selection import train_test_split

X_train_set , X_val, y_train_set, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=2)
print ("No. of images in train dataset: ", len(X_train_set))
print ("No. of images in Validation dataset: ", len(X_val))

print ("X_train_set Shape: ", X_train_set.shape)
print ("X_val Shape: ", X_val.shape)
print("y_train_set Shape: ", y_train_set.shape)
print("y_val Shape: ", y_val.shape)

tf.set_random_seed(42)

# building basic Cnn model

#Import necessary libraries to build the model...
from keras.models import Sequential
from keras.layers import Convolution2D, Dropout, Dense
from keras.layers import BatchNormalization
from keras.layers import MaxPooling2D
from keras.layers import Flatten
from keras.optimizers import adam
from keras.optimizers import sgd
from keras.layers import LeakyReLU

model1 = Sequential()
model1.add(BatchNormalization(input_shape = (128,128,3)))
model1.add(Convolution2D(32, (3,3), activation ='relu', input_shape = (128, 128, 3))) 
model1.add(MaxPooling2D(pool_size=2))

model1.add(Convolution2D(filters=64, kernel_size=4, padding='same', activation='relu'))
model1.add(MaxPooling2D(pool_size=2))


model1.add(Convolution2D(filters=128, kernel_size=3, padding='same', activation='relu'))
model1.add(MaxPooling2D(pool_size=2))


model1.add(Convolution2D(filters=128, kernel_size=2, padding='same', activation='relu'))
model1.add(MaxPooling2D(pool_size=2))


model1.add(Flatten()) 

# fully connected layer
model1.add(Dense(units=128,activation = 'relu'))
model1.add(Dense(units = 64, activation = 'relu'))

model1.add(Dense(units = 32, activation = 'relu'))

model1.add(Dense(units = 12, activation = 'softmax'))

model1.compile(optimizer='adam', loss = 'categorical_crossentropy',metrics = ['accuracy'])

model1.summary()

hist_model = model1.fit(X_train_set,y_train_set,
                    epochs=30, 
                    validation_data=(X_val,y_val),
                    verbose = 1,
                    initial_epoch=0)

# building another model with leakyrelu as th activation function

model2 = Sequential()
model2.add(BatchNormalization(input_shape = (128,128,3)))
model2.add(Convolution2D(32, (3,3), input_shape = (128, 128, 3))) 
model2.add(LeakyReLU(alpha=0.1))
model2.add(MaxPooling2D(pool_size=2))


model2.add(Convolution2D(filters=64, kernel_size=5, padding='same'))
model2.add(LeakyReLU(alpha=0.1))
model2.add(MaxPooling2D(pool_size=2))


model2.add(Convolution2D(filters=128, kernel_size=4, padding='same'))
model2.add(LeakyReLU(alpha=0.1))
model2.add(MaxPooling2D(pool_size=2))


model2.add(Convolution2D(filters=128, kernel_size=3, padding='same'))
model2.add(LeakyReLU(alpha=0.1))
model2.add(MaxPooling2D(pool_size=2))


model2.add(Convolution2D(filters=128, kernel_size=2, padding='same'))
model2.add(LeakyReLU(alpha=0.1))
model2.add(MaxPooling2D(pool_size=2))


model2.add(Flatten()) 

# fully connected layer
model2.add(Dense(units=128))
model2.add(LeakyReLU(alpha=0.1))
model2.add(Dense(units = 64))
model2.add(LeakyReLU(alpha=0.1))

model2.add(Dense(units = 32))
model2.add(LeakyReLU(alpha=0.1))

model2.add(Dense(units = 12, activation = 'softmax'))

model2.compile(optimizer='adam', loss = 'categorical_crossentropy',metrics = ['accuracy'])

model2.summary()

hist_model2 = model2.fit(X_train_set,y_train_set,
                    epochs=30, 
                    validation_data=(X_val,y_val),
                    verbose = 1,
                    initial_epoch=0)

# accuracy has minutely improved

# Running multiple experiments

# Trying to optimise kernel initailizer using grid search

from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import GridSearchCV

def create_model(init_mode='uniform'):
  model = Sequential()
  model.add(BatchNormalization(input_shape = (128,128,3)))
  model.add(Convolution2D(32, (3,3), input_shape = (128, 128, 3), kernel_initializer = init_mode)) 
  model.add(LeakyReLU(alpha=0.1))
  model.add(MaxPooling2D(pool_size=2))
  

  model.add(Convolution2D(filters=64, kernel_size=5, padding='same', kernel_initializer = init_mode))
  model.add(LeakyReLU(alpha=0.1))
  model.add(MaxPooling2D(pool_size=2))
 

  model.add(Convolution2D(filters=128, kernel_size=4, padding='same', kernel_initializer = init_mode))
  model.add(LeakyReLU(alpha=0.1))
  model.add(MaxPooling2D(pool_size=2))
 
  model.add(Convolution2D(filters=128, kernel_size=3, padding='same', kernel_initializer = init_mode))
  model.add(LeakyReLU(alpha=0.1))
  model.add(MaxPooling2D(pool_size=2))
 

  model.add(Convolution2D(filters=128, kernel_size=2, padding='same', kernel_initializer = init_mode))
  model.add(LeakyReLU(alpha=0.1))
  model.add(MaxPooling2D(pool_size=2))

  model.add(Flatten()) 

  # fully connected layer
  model.add(Dense(units=128, kernel_initializer = init_mode))
  model.add(LeakyReLU(alpha=0.1))
  model.add(Dense(units = 64, kernel_initializer = init_mode))
  model.add(LeakyReLU(alpha=0.1))
)
  model.add(Dense(units = 32, kernel_initializer = init_mode))
  model.add(LeakyReLU(alpha=0.1))

  model.add(Dense(units = 12, activation = 'softmax', kernel_initializer = init_mode)) 
  model.compile(optimizer='adam', loss = 'categorical_crossentropy',metrics = ['accuracy'])
  return model

# create model
modelOp = KerasClassifier(build_fn=create_model, epochs=10, verbose=0)

# define the grid search parameters
init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']
param_grid = dict(init_mode=init_mode)

grid = GridSearchCV(estimator=modelOp, param_grid=param_grid, cv=2)
grid_result = grid.fit(X_train_set, y_train_set)

# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))

means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))

# optimizing batch size and number of epochs using grid search

from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import GridSearchCV

def create_model(init_mode='uniform'):
  model = Sequential()
  model.add(BatchNormalization(input_shape = (128,128,3)))
  model.add(Convolution2D(32, (3,3), input_shape = (128, 128, 3), kernel_initializer = 'lecun_uniform')) 
  model.add(LeakyReLU(alpha=0.1))
  model.add(MaxPooling2D(pool_size=2))
 

  model.add(Convolution2D(filters=64, kernel_size=5, padding='same', kernel_initializer = 'lecun_uniform'))
  model.add(LeakyReLU(alpha=0.1))
  model.add(MaxPooling2D(pool_size=2))
 

  model.add(Convolution2D(filters=128, kernel_size=4, padding='same', kernel_initializer = 'lecun_uniform'))
  model.add(LeakyReLU(alpha=0.1))
  model.add(MaxPooling2D(pool_size=2))
  

  model.add(Convolution2D(filters=128, kernel_size=3, padding='same', kernel_initializer = 'lecun_uniform'))
  model.add(LeakyReLU(alpha=0.1))
  model.add(MaxPooling2D(pool_size=2))
 

  model.add(Convolution2D(filters=128, kernel_size=2, padding='same', kernel_initializer = 'lecun_uniform'))
  model.add(LeakyReLU(alpha=0.1))
  model.add(MaxPooling2D(pool_size=2))
  

  model.add(Flatten()) 

  # fully connected layer
  model.add(Dense(units=128, kernel_initializer = 'lecun_uniform'))
  model.add(LeakyReLU(alpha=0.1))
  model.add(Dense(units = 64, kernel_initializer = 'lecun_uniform'))
  model.add(LeakyReLU(alpha=0.1))
 
  model.add(Dense(units = 32, kernel_initializer = 'lecun_uniform'))
  model.add(LeakyReLU(alpha=0.1))

  model.add(Dense(units = 12, activation = 'softmax')) 
  model.compile(optimizer='adam', loss = 'categorical_crossentropy',metrics = ['accuracy'])
  return model

# create model
modelOp = KerasClassifier(build_fn=create_model, verbose=0)

# define the grid search parameters
batch_size = [ 40, 60, 80]
epochs = [20, 30, 50]
param_grid = dict(batch_size=batch_size, epochs=epochs)

grid = GridSearchCV(estimator=modelOp, param_grid=param_grid, n_jobs=1, scoring="accuracy", cv=2)
grid_result = grid.fit(X_train_set, np.argmax(y_train_set, axis=1))

# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))

# final model based on the results of the abpve grid searches

model_f = Sequential()
model_f.add(BatchNormalization(input_shape = (128,128,3)))
model_f.add(Convolution2D(32, (3,3), input_shape = (128, 128, 3), kernel_initializer = 'lecun_uniform')) 
model_f.add(LeakyReLU(alpha=0.1))
model_f.add(MaxPooling2D(pool_size=2))


model_f.add(Convolution2D(filters=64, kernel_size=5, padding='same', kernel_initializer = 'lecun_uniform'))
model_f.add(LeakyReLU(alpha=0.1))
model_f.add(MaxPooling2D(pool_size=2))


model_f.add(Convolution2D(filters=128, kernel_size=4, padding='same', kernel_initializer = 'lecun_uniform'))
model_f.add(LeakyReLU(alpha=0.1))
model_f.add(MaxPooling2D(pool_size=2))


model_f.add(Convolution2D(filters=128, kernel_size=3, padding='same', kernel_initializer = 'lecun_uniform'))
model_f.add(LeakyReLU(alpha=0.1))
model_f.add(MaxPooling2D(pool_size=2))


model_f.add(Convolution2D(filters=128, kernel_size=2, padding='same', kernel_initializer = 'lecun_uniform'))
model_f.add(LeakyReLU(alpha=0.1))
model_f.add(MaxPooling2D(pool_size=2))


model_f.add(Flatten()) 

# fully connected layer
model_f.add(Dense(units=128, kernel_initializer = 'lecun_uniform'))
model_f.add(LeakyReLU(alpha=0.1))
model_f.add(Dense(units = 64, kernel_initializer = 'lecun_uniform'))
model_f.add(LeakyReLU(alpha=0.1))

model_f.add(Dense(units = 32, kernel_initializer = 'lecun_uniform'))
model_f.add(LeakyReLU(alpha=0.1))
#output layer
model_f.add(Dense(units = 12, activation = 'softmax'))

model_f.compile(optimizer='adam', loss = 'categorical_crossentropy',metrics = ['accuracy'])

model_f.summary()

hist_model_f = model_f.fit(X_train_set,y_train_set,
                    epochs=50, 
                    validation_data=(X_val,y_val),
                    verbose = 1,
                    initial_epoch=0, batch_size=60)

# Predict the accuracy

# predictions and evaluation and classification report for validation data

predictions_train = model_f.predict(X_train_set)

from sklearn import metrics
print ("Train Accuracy: ", metrics.accuracy_score(np.argmax(y_train_set, axis=1), np.argmax(predictions_train, axis=1)))

from sklearn.metrics import classification_report

print ("Classification Report for train data")
print(classification_report(np.argmax(y_train_set, axis=1), np.argmax(predictions_train, axis=1)))

# predictions and evaluation and classification report for validation data

predictions_val = model_f.predict(X_val)

print ("Val Accuracy: ", metrics.accuracy_score(np.argmax(y_val, axis=1), np.argmax(predictions_val, axis=1)))

print ("Classification Report for Validation data")
print(classification_report(np.argmax(y_val, axis=1), np.argmax(predictions_val, axis=1)))

#The category 0 and 6 have very less f1 score.  the model can be made to learn more on these categories.  more augmentation in these categories can be done and  model can be made learn better

# Evaluate Model on Test Set

X_test = np.array(X_test)

predictions_= model_f.predict(X_test)

FinalPred = np.argmax(predictions_, axis=1)

FinalPred

y_test = le.inverse_transform(FinalPred)

y_test

X_test_name = np.array(X_test_name)
y_test = np.array(y_test)
dataset = pd.DataFrame({'file': X_test_name, 'species': y_test}, columns=['file', 'species'])

dataset.head()

dataset.to_csv("/content/drive/My Drive/Great Learning Residency 7.2 CNN/Project 1/ResultsPlantClassification.csv", index=False)

# there was a problem uploading results to kaggle to get test accuracy . Similiar problem was there will downloading the dataset using import kaggle